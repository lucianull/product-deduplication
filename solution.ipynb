{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Solution Approach</h2>\n",
    "\n",
    "To tackle the problem of product deduplication, I propose using a vector database. Since web crawling and scraping can generate large volumes of product data, an efficient method for querying and matching duplicate products is essential. A vector database enables fast similarity searches, making it a suitable choice for this task.\n",
    "\n",
    "<h3>Step 1: Data Cleaning and Preparation</h3>\n",
    "The first step in this solution is to clean the dataset, ensuring that the product attributes are structured and free of inconsistencies. This preprocessing is crucial for obtaining meaningful embeddings.\n",
    "\n",
    "<h3>Step 2: Generating Product Embeddings</h3>\n",
    "Once the data is cleaned, we generate embeddings for each product. These embeddings serve as dense vector representations that capture the semantic meaning of product attributes, making it easier to compare products. The embeddings will be stored in the vector database for efficient retrieval.\n",
    "\n",
    "<h3>Step 3: Identifying Duplicates</h3>\n",
    "To detect duplicate products, we perform a nearest-neighbor search on the stored embeddings. For each product, we retrieve its 10 closest neighbors, assuming that the dataset is not large enough to require a higher threshold. We then compute the cosine similarity between the embeddings. If the similarity score indicates a high likelihood of duplication, the products are merged into a single enriched entry.\n",
    "\n",
    "<h3>Alternative Approach: Siamese Network</h3>\n",
    "Another possible solution would be to train a Siamese network, using cosine similarity between product embeddings as a basis for generating labels. This approach could improve the accuracy of duplicate detection. However, it would be impractical in our case, as each newly added product would need to be compared against all existing products, making it computationally expensive.\n",
    "\n",
    "By leveraging a vector database and embedding-based similarity search, this solution ensures efficient deduplication while maximizing the available product information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Choosing Features for Embeddings</h3>\n",
    "\n",
    "To generate meaningful embeddings, I selected a subset of product features, specifically the text-based attributes. These fields provide essential context for the embedding model, allowing it to accurately capture the characteristics of each product. By focusing on textual data, we ensure that the model learns a rich semantic representation, improving the effectiveness of similarity searches.\n",
    "\n",
    "Certain features, such as domain, page URL, and intended industries, could introduce noise into the embeddings, making it harder to match products accurately. Instead, a more advanced approach—given greater computational resources and enhanced web scraping capabilities—would involve extracting product images and combining them with text. This could be achieved using a model like <strong>CLIP</strong>, which aligns text and image embeddings into a shared space, enabling more robust and precise product matching.\n",
    "\n",
    "I combined the text data, converted it to lowercase, and removed all special characters that are not alphanumeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved as data/cleaned_products.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the Parquet file\n",
    "parquet_file = \"data/veridion_product_deduplication_challenge.snappy.parquet\"\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Define text columns to concatenate\n",
    "text_columns = ['product_title', 'product_summary', 'product_name', 'brand', 'unspsc', 'description']\n",
    "\n",
    "# Function to remove punctuation and unknown characters\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Fill missing values and concatenate text fields\n",
    "df['combined_text'] = df[text_columns].fillna('').apply(\n",
    "    lambda row: ' '.join(row.astype(str)).strip().lower(), axis=1\n",
    ")\n",
    "\n",
    "# Apply punctuation removal\n",
    "df['combined_text'] = df['combined_text'].apply(remove_punctuation)\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['product_name', 'combined_text']]\n",
    "\n",
    "# Reset index and add an ID column\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['id'] = df.index + 1\n",
    "\n",
    "# Replace empty or whitespace-only product names with \"no_name\"\n",
    "df['product_name'] = df['product_name'].apply(lambda x: x.strip() if isinstance(x, str) and x.strip() else \"no_name\")\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['id', 'product_name', 'combined_text']]\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = \"data/cleaned_products.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"CSV file saved as {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating Product Embeddings</h3>\n",
    "\n",
    "To generate product embeddings, I use a sentence transformer model that processes the combined textual attributes of each product. Specifically, I use the \"all-mpnet-base-v2\" model from Sentence Transformers, which produces 768-dimensional embeddings.\n",
    "\n",
    "The process involves:\n",
    "<ol>\n",
    "    <li>Tokenizing the text with padding and truncation.</li>\n",
    "    <li>Passing it through the transformer model to obtain contextualized token embeddings.</li>\n",
    "    <li>Computing a sentence-level embedding by taking a weighted mean of token embeddings, using the attention mask to ignore padding tokens.</li>\n",
    "</ol>\n",
    "This approach ensures that each product is represented in a dense vector space, capturing its semantic meaning for effective similarity searches. The generated embeddings are then stored for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucia\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMPNetModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFMPNetModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMPNetModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMPNetModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMPNetModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure TensorFlow uses GPU if available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Model for embeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    attention_mask = tf.cast(inputs[\"attention_mask\"], tf.float32)\n",
    "    input_mask_expanded = tf.expand_dims(attention_mask, -1)\n",
    "    sum_embeddings = tf.reduce_sum(token_embeddings * input_mask_expanded, axis=1)\n",
    "    sum_mask = tf.reduce_sum(input_mask_expanded, axis=1)\n",
    "    embedding = sum_embeddings / sum_mask\n",
    "    return embedding[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 21946/21946 [1:32:47<00:00,  3.94product/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to product_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/cleaned_products.csv\")\n",
    "\n",
    "# Generate embeddings with progress bar\n",
    "embeddings = []\n",
    "for text in tqdm(data[\"combined_text\"], desc=\"Generating Embeddings\", unit=\"product\"):\n",
    "    embeddings.append(get_embedding(text).tolist())\n",
    "\n",
    "# Save embeddings to a new CSV\n",
    "df_embeddings = pd.DataFrame({\"id\": data[\"id\"], \"product_name\": data[\"product_name\"], \"embedding\": embeddings})\n",
    "df_embeddings.to_csv(\"data/product_embeddings.csv\", index=False)\n",
    "print(\"Saved embeddings to product_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Storing Embeddings in a Vector Database</h3>\n",
    "\n",
    "The next step is to store the generated embeddings in a vector database for efficient similarity searches. I chose ChromaDB due to its simplicity and ease of use, making it a great option for quickly setting up a product-matching pipeline.\n",
    "\n",
    "However, other vector databases, such as FAISS, Milvus, or Weaviate, could also be used depending on scalability and performance requirements. The choice of database depends on factors like dataset size, query speed, and integration needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import ast\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data/product_embeddings.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert embedding column (assuming it's stored as a string in CSV)\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products stored successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"products\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# Insert products into ChromaDB\n",
    "collection.add(\n",
    "    ids=df[\"id\"].astype(str).tolist(),  \n",
    "    embeddings=df[\"embedding\"].tolist(),\n",
    "    metadatas=[{\"product_name\": name} for name in df[\"product_name\"]],\n",
    ")\n",
    "\n",
    "print(\"Products stored successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Consolidating Duplicates</h3>\n",
    "\n",
    "The final step is to iterate over each product and check its nearest neighbors in the vector database. If a product is identified as a duplicate, it is marked as seen to ensure that we skip it in future iterations (since we have already grouped it with its duplicates).\n",
    "\n",
    "To determine duplicates, I set a cosine distance threshold of 0.15. Based on initial testing, this threshold strikes a balance between minimizing false negatives (missed duplicates) while avoiding excessive false positives.\n",
    "\n",
    "For each identified duplicate, we merge their IDs and product names, creating a single enriched entry that consolidates all available information. The final dataset is saved for further analysis and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing products: 100%|██████████| 21946/21946 [04:37<00:00, 79.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate consolidation complete! Saved as 'products_output_0_15.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def consolidate_duplicates(threshold=0.15, top_k=10):\n",
    "    seen = set()\n",
    "    consolidated = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing products\"):\n",
    "        query_id = str(row[\"id\"])\n",
    "        query_embedding = row[\"embedding\"]\n",
    "\n",
    "        if query_id in seen:\n",
    "            continue  # Skip already processed duplicates\n",
    "\n",
    "        # Search for similar products\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k\n",
    "        )\n",
    "\n",
    "        matched_ids = results[\"ids\"][0]\n",
    "        distances = results[\"distances\"][0]\n",
    "\n",
    "        # Find duplicates within threshold\n",
    "        duplicates = [query_id]\n",
    "        for match_id, distance in zip(matched_ids, distances):\n",
    "            if match_id != query_id and distance < threshold:\n",
    "                duplicates.append(match_id)\n",
    "                seen.add(match_id)\n",
    "\n",
    "        # Merge product ids and names\n",
    "        merged_ids = \" / \".join(duplicates)\n",
    "        merged_name = \" / \".join(df[df[\"id\"].astype(str).isin(duplicates)][\"product_name\"].tolist())\n",
    "\n",
    "        consolidated.append({\n",
    "            \"ids\": merged_ids,\n",
    "            \"product_name\": merged_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(consolidated)\n",
    "\n",
    "cleaned_df = consolidate_duplicates()\n",
    "\n",
    "cleaned_df.to_csv(\"data/products_output_0_15.csv\", index=False)\n",
    "print(\"Duplicate consolidation complete! Saved as 'products_output_0_15.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromadb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
